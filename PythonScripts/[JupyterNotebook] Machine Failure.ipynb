{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4deb575-4a85-491d-9dcc-15bf57fdee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 07:48:53.616485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-25 07:48:53.947456: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from plot_keras_history import plot_history\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "#Config----------------------------------------------------------------------------------------------------------------------------\n",
    "#All data is gathered  by this func\n",
    "def get_config(config_filename):\n",
    "\n",
    "    with open(config_filename, 'r',encoding='UTF8') as f:\n",
    "        config_enkispathdir = f.readline().strip()\n",
    "        config_datapathdir = f.readline().strip()\n",
    "        config_resultpathdir = f.readline().strip()\n",
    "        config_modelpathdir = f.readline().strip()\n",
    "    \n",
    "    config_enkispathdir = config_enkispathdir.split('=')[1].strip()\n",
    "    config_datapathdir = config_datapathdir.split('=')[1].strip()\n",
    "    config_resultpathdir = config_resultpathdir.split('=')[1].strip()\n",
    "    config_modelpathdir = config_modelpathdir.split('=')[1].strip()\n",
    "        \n",
    "    conf = edict()\n",
    "    \n",
    "    conf.enkis_path = Path(config_enkispathdir)\n",
    "    conf.data_path = Path(config_datapathdir)\n",
    "    conf.result_data_path = Path(config_resultpathdir)\n",
    "    conf.model_path = Path(config_modelpathdir)\n",
    "    \n",
    "    conf.npy_data_path = Path(conf.data_path/'npy')\n",
    "    conf.pickle_data_path = Path(conf.data_path/'pickles')\n",
    "    conf.original_data_path = Path(conf.data_path/'original_data')\n",
    "    conf.update_data_path = Path(conf.data_path/'update_data')\n",
    "    conf.original_site_data_pickle = 'location_dict.pickle'\n",
    "    conf.all_site_data_pickle = 'update_location_dict.pickle'\n",
    "\n",
    "    conf.pre_pickle = conf.pickle_data_path/'data_pump_pres_dtonic01.pickle'\n",
    "    conf.pua_pickle = conf.pickle_data_path/'data_pump_acur_dtonic01.pickle'\n",
    "    conf.sea_pickle = conf.pickle_data_path/'data_sensor_acur_dtonic01.pickle'\n",
    "\n",
    "    conf.pre_label_pickle = conf.pickle_data_path/'data_label_pre.pickle'\n",
    "    conf.pua_label_pickle = conf.pickle_data_path/'data_label_pua.pickle'\n",
    "    conf.sea_label_pickle = conf.pickle_data_path/'data_label_sea.pickle'\n",
    "    conf.eventlist_path = conf.data_path/'장애리스트.csv'\n",
    "    \n",
    "    conf.device_report_path = conf.result_data_path/'DEVICE'\n",
    "    conf.pump_report_path = conf.device_report_path/'PUMP'\n",
    "    conf.sensor_report_path = conf.device_report_path/'SENSOR'\n",
    "    conf.plot_result_path = conf.result_data_path/'plots'\n",
    "    \n",
    "    conf.pvalue = [12, 24]\n",
    "    conf.temp_value = [20, 40, -15, -20]\n",
    "    conf.hum_value = [25, 40, -15, -25]\n",
    "    conf.pre_value = [20, 50, 1535.0, 1590.0]\n",
    "    conf.pua_value = [0, 30, 1200.0, 2950.0]\n",
    "    conf.sea_value = [8845, 10361]\n",
    "    \n",
    "    conf.pm25_max = 300.0 \n",
    "    conf.pm25_min = -250.0\n",
    "    conf.pm10_max = 2400.0\n",
    "    conf.pm10_min = -2000.0\n",
    "    conf.pm_dataset_path = conf.pickle_data_path/'pm_dataset.pickle'\n",
    "    conf.pm_TSNE_path = conf.pickle_data_path/'pm_tsne_2d.pickle'\n",
    "    \n",
    "    conf.sensor_model_path = conf.model_path/'dtonic.hdf5'\n",
    "    conf.pm_model_path = conf.model_path/'pm_dnn.hdf5'\n",
    "\n",
    "    return conf\n",
    "\n",
    "def edict2dict(edict_obj):\n",
    "    dict_obj = {}\n",
    "    for key, vals in edict_obj.items():\n",
    "        if isinstance(vals, edict):\n",
    "            dict_obj[key] = edict2dict(vals)\n",
    "        else:\n",
    "            dict_obj[key] = vals\n",
    "    return dict_obj\n",
    "\n",
    "#DataReader----------------------------------------------------------------------------------------------------------------------------\n",
    "#Used for reading the data into the modules\n",
    "def load_site_data(conf):\n",
    "\n",
    "    all_data_path = os.path.join(conf.pickle_data_path, conf.all_site_data_pickle)\n",
    "    ## data load\n",
    "    with open(all_data_path, 'rb') as fptr:\n",
    "        site_data_dict = pickle.load(fptr)\n",
    "        \n",
    "    return site_data_dict\n",
    "       \n",
    "def get_column_dataframe(data_dict, col='', P_signal=False):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    flag = 0\n",
    "\n",
    "    for key in tqdm(data_dict.keys()):\n",
    "        data_col = data_dict[key]\n",
    "\n",
    "        if P_signal:\n",
    "            data_col = data_col[data_col[col] == -100]\n",
    "        else:\n",
    "            data_col = data_col[data_col[col] != -100]\n",
    "\n",
    "        data_col['t_index'] = data_col.time.apply(lambda _: get_time(_))\n",
    "        data_col['t_index'] = pd.to_datetime(data_col['t_index'], format='%Y-%m-%d-%H-%M')\n",
    "        data_col[key] = data_col[col]\n",
    "        data_col = data_col.set_index('t_index')\n",
    "\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            data = data_col[key]\n",
    "        else:\n",
    "            data = pd.concat([data, data_col[key]], axis=1)\n",
    "\n",
    "    data_med = data.resample('1D').mean()\n",
    "    med = data_med.median(axis=1)\n",
    "\n",
    "    return data, med    \n",
    "    \n",
    "def get_p_signal(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']      \n",
    "    P_signal_df, _ = get_column_dataframe(site_dict, col ='TEMP', P_signal=True) # col name is not important\n",
    "    #limit time periods\n",
    "    P_signal = set_time_period(P_signal_df, date_from, date_to)\n",
    "    return P_signal\n",
    "    \n",
    "def get_temperature_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    temp_df, temp_med = get_column_dataframe(site_dict, col ='TEMP', P_signal=False) \n",
    "    temp_df = set_time_period(temp_df, date_from, date_to)\n",
    "    temp_med = set_time_period(temp_med, date_from, date_to)\n",
    "    save_data_pickle(conf, temp_df, 'Temp', 'dtonic01')\n",
    "    save_data_pickle(conf, temp_med, 'Temp_median', 'dtonic01')    \n",
    "    return (temp_df, temp_med)\n",
    "    \n",
    "def get_humidity_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    hum_df, hum_med = get_column_dataframe(site_dict, col ='HUM', P_signal=False) \n",
    "    hum_df = set_time_period(hum_df, date_from, date_to)\n",
    "    hum_med = set_time_period(hum_med, date_from, date_to)   \n",
    "    save_data_pickle(conf, hum_df, 'Humid', 'dtonic01')\n",
    "    save_data_pickle(conf, hum_med, 'Humid_median', 'dtonic01')\n",
    "    return (hum_df, hum_med)\n",
    "    \n",
    "def get_pump_pressure_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    pre_df, pre_med = get_column_dataframe(site_dict, col ='PRE', P_signal=False) # Pump Pressure \n",
    "\n",
    "    if (date_from == \"none\") and (date_to == \"none\"):\n",
    "        save_data_pickle(conf, pre_df, 'pump_pres', 'dtonic01')\n",
    "        save_data_pickle(conf, pre_med, 'pump_pres_median', 'dtonic01')\n",
    "        return (pre_df, pre_med) \n",
    "    else:\n",
    "        pre_df = set_time_period(pre_df, date_from, date_to)\n",
    "        pre_med = set_time_period(pre_med, date_from, date_to)\n",
    "        save_data_pickle(conf, pre_df, 'pump_pres', 'dtonic01')\n",
    "        save_data_pickle(conf, pre_med, 'pump_pres_median', 'dtonic01')\n",
    "        return (pre_df, pre_med) \n",
    "\n",
    "def get_pump_current_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    pua_df, pua_med = get_column_dataframe(site_dict, col ='PUA', P_signal=False) # Pump Current\n",
    "    \n",
    "    if (date_from == \"none\") and (date_to == \"none\"):\n",
    "        save_data_pickle(conf, pua_df, 'pump_acur', 'dtonic01')\n",
    "        save_data_pickle(conf, pua_med, 'pump_acur_median', 'dtonic01')\n",
    "        return (pua_df, pua_med)\n",
    "    else:\n",
    "        pua_df = set_time_period(pua_df, date_from, date_to)\n",
    "        pua_med = set_time_period(pua_med, date_from, date_to)\n",
    "        save_data_pickle(conf, pua_df, 'pump_acur', 'dtonic01')\n",
    "        save_data_pickle(conf, pua_med, 'pump_acur_median', 'dtonic01')\n",
    "        return (pua_df, pua_med)\n",
    "\n",
    "def get_sensor_current_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    sea_df, sea_med = get_column_dataframe(site_dict, col ='SEA', P_signal=False) # Sensor Current \n",
    "    \n",
    "    if (date_from == \"none\") and (date_to == \"none\"):\n",
    "        save_data_pickle(conf, sea_df, 'sensor_acur', 'dtonic01')\n",
    "        save_data_pickle(conf, sea_med, 'sensor_acur_median', 'dtonic01')\n",
    "        return (sea_df, sea_med)\n",
    "    else:\n",
    "        sea_df = set_time_period(sea_df, date_from, date_to)\n",
    "        sea_med = set_time_period(sea_med, date_from, date_to)\n",
    "        save_data_pickle(conf, sea_df, 'sensor_acur', 'dtonic01')\n",
    "        save_data_pickle(conf, sea_med, 'sensor_acur_median', 'dtonic01')\n",
    "        return (sea_df, sea_med)\n",
    "    \n",
    "def get_pm25_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    pm25_df, pm25_med = get_column_dataframe(site_dict, col ='PM2.5', P_signal=False) \n",
    "    pm25_df = set_time_period(pm25_df, date_from, date_to)\n",
    "    pm25_med = set_time_period(pm25_med, date_from, date_to)\n",
    "    save_data_pickle(conf, pm25_df, 'PM2.5', 'dtonic01')\n",
    "    save_data_pickle(conf, pm25_med, 'PM2.5_median', 'dtonic01')    \n",
    "    return (pm25_df, pm25_med)\n",
    "\n",
    "def get_pm10_data(site_dict, conf, date_from, date_to):\n",
    "    # col is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']    \n",
    "    pm10_df, pm10_med = get_column_dataframe(site_dict, col ='PM10', P_signal=False) \n",
    "    pm10_df = set_time_period(pm10_df, date_from, date_to)\n",
    "    pm10_med = set_time_period(pm10_med, date_from, date_to)\n",
    "    save_data_pickle(conf, pm10_df, 'PM10', 'dtonic01')\n",
    "    save_data_pickle(conf, pm10_med, 'PM10_median', 'dtonic01')\n",
    "    return (pm10_df, pm10_med)\n",
    "\n",
    "def get_all_data(site_dict, conf, date_from, date_to):\n",
    "   \n",
    "    P_signal = get_p_signal(site_dict, conf, date_from, date_to)\n",
    "    temp = get_temperature_data(site_dict, conf, date_from, date_to)\n",
    "    hum = get_humidity_data(site_dict, conf, date_from, date_to)\n",
    "    pre = get_pump_pressure_data(site_dict, conf, date_from, date_to)\n",
    "    pua = get_pump_current_data(site_dict, conf, date_from, date_to)\n",
    "    sea = get_sensor_current_data(site_dict, conf, date_from, date_to)\n",
    "    pm25 = get_pm25_data(site_dict, conf, date_from, date_to)\n",
    "    pm10 = get_pm10_data(site_dict, conf, date_from, date_to)\n",
    "    \n",
    "    return P_signal, temp, hum, pre, pua, sea, pm25, pm10\n",
    "\n",
    "def get_control_parameters():    \n",
    "    with open('ai_voucher/enkis/config.txt', 'r',encoding='UTF8') as f:\n",
    "        f.readline()\n",
    "        p_value = f.readline().strip()\n",
    "        f.readline()\n",
    "        temp_value = f.readline().strip()\n",
    "        f.readline()\n",
    "        hum_value = f.readline().strip()\n",
    "        f.readline()\n",
    "        pre_value = f.readline().strip()\n",
    "        f.readline()\n",
    "        pua_value = f.readline().strip()\n",
    "        f.readline()\n",
    "        sea_value = f.readline().strip()\n",
    "\n",
    "    p_value = list(map(float,(p_value.split(':')[1].split(','))))\n",
    "    temp_value = list(map(float,(temp_value.split(':')[1].split(','))))\n",
    "    hum_value = list(map(float,(hum_value.split(':')[1].split(','))))\n",
    "    pre_value = list(map(float,(pre_value.split(':')[1].split(','))))\n",
    "    pua_value = list(map(float,(pua_value.split(':')[1].split(','))))\n",
    "    sea_value = list(map(float,(sea_value.split(':')[1].split(','))))\n",
    "    \n",
    "    return p_value, temp_value, hum_value, pre_value, pua_value, sea_value   \n",
    "\n",
    "#Utils----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_time(time_stamp):\n",
    "    temp = str(time_stamp).split(':')\n",
    "    ret = str(temp[0]).replace(' ', '-') + '-' + str(temp[1])\n",
    "    return ret\n",
    "    \n",
    "def set_time_period(data_df, date_from, date_to):\n",
    "    return data_df.loc[str(date_from):str(date_to)]\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + directory)\n",
    "        \n",
    "def get_event_list(conf):\n",
    "    issuelist_df = pd.read_csv(conf.eventlist_path, sep = '\\s*,\\s*', engine='python')\n",
    "    events_df = issuelist_df.loc[:, ['간이측정소명', 'start', 'end', '장애유형']]\n",
    "    events_df['장애유형'] = events_df['장애유형'].str.replace(' ', '')\n",
    "    events_df['간이측정소명'] = events_df['간이측정소명'].str.replace('&', ',')\n",
    "    \n",
    "    return events_df\n",
    "    \n",
    "def save_data_pickle(conf, data=None, colName=None, postName=None):\n",
    "    # colName is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']   \n",
    "    if colName is not None and data is not None:\n",
    "        save_data_path = os.path.join(conf.pickle_data_path, 'data_{}_{}.pickle'.format(colName,postName))    \n",
    "        with open(save_data_path, 'wb') as fptr:\n",
    "            pickle.dump(data, fptr)  \n",
    "    else:\n",
    "        print('Wrong command parameters')\n",
    "        \n",
    "def load_data_pickle(conf, colName=None, postName=None):\n",
    "    # colName is in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE']   \n",
    "    if colName is not None:\n",
    "        load_data_path = os.path.join(conf.pickle_data_path, 'data_{}_{}.pickle'.format(colName,postName))\n",
    "        with open(load_data_path,'rb') as fptr:\n",
    "            data_df = pickle.load(fptr)\n",
    "        return data_df\n",
    "    else:\n",
    "        print('Wrong command parameters')\n",
    "        return None\n",
    "        \n",
    "def convert_dataframe_to_list(event_df):\n",
    "    eventDict = event_df.to_dict('split')  \n",
    "    return eventDict['data']\n",
    "\n",
    "def s_daily_average(data_dict, moving_value=3, spilt_value=7):\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    for key in data_dict.keys():\n",
    "        sample = data_dict[key]\n",
    "        daily_values = sample.resample('1D')\n",
    "        daily_v_indeces = (daily_values.max()+daily_values.median())/2\n",
    "            \n",
    "        week_data = []\n",
    "        count = len(daily_v_indeces)\n",
    "        for i in range(count-spilt_value):\n",
    "            week_sample = daily_v_indeces.iloc[i:i+spilt_value]           \n",
    "            week_data.append(week_sample)\n",
    "        if week_data is not None:\n",
    "            data[key] = week_data\n",
    "        \n",
    "    return data\n",
    "    \n",
    "    \n",
    "#-----------------------------------------------\n",
    "#Raw Data Preprocessing\n",
    "#-----------------------------------------------\n",
    "def RunPreproc(conf):\n",
    "    # data (excel) 경로\n",
    "    file_list = glob.glob(str(conf.original_data_path)+'/*.xlsx')\n",
    "    \n",
    "    # data 전처리\n",
    "    def data_set(file_list):\n",
    "\n",
    "        data = dict()\n",
    "        for file_name in tqdm(file_list):\n",
    "\n",
    "            # (excel) file open\n",
    "            df  = pd.read_excel(file_name, engine='openpyxl')\n",
    "            # 측정장소명 key에 저장\n",
    "            key = df['측정소'].unique()[0]\n",
    "            # 하단의 data 통계 수치 제거\n",
    "            df  = df.iloc[:-3]\n",
    "            # 시간 정보의 date, time 컬럼 생성 및, 미수신 신호 -100 변환\n",
    "            df = df.astype({'년': 'int', '월': 'int', '일': 'int', '시': 'int', '분': 'int'})\n",
    "            df = df.replace({'(P)': -100})\n",
    "\n",
    "            df['date'] = df['년'].map(str) + '-' + df['월'].map(str) + '-' + df['일'].map(str)\n",
    "            df['time'] = df['년'].map(str) + '-' + df['월'].map(str) + '-' + df['일'].map(str) + ' ' + df[\n",
    "                '시'].map(str) + ':' + df['분'].map(str)\n",
    "            df = df.reset_index(drop=True)\n",
    "            df.drop(['년', '월', '일', '시', '분', '측정소'], axis=1, inplace=True)\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "            df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M')\n",
    "\n",
    "            # 최종 output : 장소를 key, 센서 데이터를 value로 가지는 dict 파일\n",
    "            data[key] = df\n",
    "        return data\n",
    "\n",
    "    location_dict = data_set(file_list)\n",
    "\n",
    "    data_name = os.path.join(conf.pickle_data_path, conf.original_site_data_pickle)\n",
    "    # pickle 파일 생성\n",
    "    with open(data_name,'wb') as f_1:\n",
    "        pickle.dump(location_dict, f_1)\n",
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "def RunPreprocWithPrevDict(conf):\n",
    "\n",
    "    # 추가 data (excel) 경로\n",
    "    file_list = glob.glob(str(conf.update_data_path)+'/*.xlsx')\n",
    "\n",
    "    # 이전 data, dict.pickle file open\n",
    "    prev_dict_name = os.path.join(conf.pickle_data_path, conf.original_site_data_pickle)\n",
    "    with open(prev_dict_name, 'rb') as f_1:\n",
    "        location_dict = pickle.load(f_1)\n",
    "\n",
    "    # data 전처리\n",
    "    def data_update(file_list,location_dict):\n",
    "        data = dict()\n",
    "        for file_name in tqdm(file_list):\n",
    "            print(\"[RunPreprocWithPrevDict] \" + file_name)\n",
    "            # (excel) file open\n",
    "            df = pd.read_excel(file_name, engine='openpyxl')\n",
    "            # 추가된 column 삭제\n",
    "            # df = df.drop(['S05', 'S0', 'L90', 'CO2', 'VIX', 'VIY', 'VIZ'], axis=1)\n",
    "\n",
    "            # 하단의 data 통계 수치 제거\n",
    "            df = df.iloc[:-3]\n",
    "            # 측정장소명 key에 저장\n",
    "            key = df['측정소'].unique()[0]\n",
    "            # 시간 정보의 date, time 컬럼 생성 및, 미수신 신호 -100 변환\n",
    "            df = df.astype({'년': 'int', '월': 'int', '일': 'int', '시': 'int', '분': 'int'})\n",
    "            df = df.replace({'(P)': -100})\n",
    "\n",
    "            df['date'] = df['년'].map(str) + '-' + df['월'].map(str) + '-' + df['일'].map(str)\n",
    "            df['time'] = df['년'].map(str) + '-' + df['월'].map(str) + '-' + df['일'].map(str) + ' ' + \\\n",
    "                              df['시'].map(str) + ':' + df['분'].map(str)\n",
    "            df = df.reset_index(drop=True)\n",
    "            df.drop(['년', '월', '일', '시', '분', '측정소'], axis=1, inplace=True)\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "            df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M')\n",
    "\n",
    "            # 최종 output : 장소를 key, 센서 데이터를 value로 가지는 dict 파일\n",
    "            if key in location_dict.keys():\n",
    "                data[key] = pd.concat([location_dict[key],df],axis= 0)\n",
    "                data[key] = data[key].reset_index(drop=True)\n",
    "            else :\n",
    "                data[key] = df\n",
    "\n",
    "        return data\n",
    "\n",
    "    update_location_dict = data_update(file_list,location_dict)\n",
    "\n",
    "    # 업데이트된 pickle 파일 생성\n",
    "    update_data_name = os.path.join(conf.pickle_data_path, conf.all_site_data_pickle)\n",
    "    with open(update_data_name,'wb') as f:\n",
    "        pickle.dump(update_location_dict,f)\n",
    "\n",
    "\n",
    "#-----------------------------------------------\n",
    "#\tGenerate Dataset\n",
    "#-----------------------------------------------\n",
    "\n",
    "def data_set_pump(data_dict, value_1,value_2,value_3,value_4):\n",
    "    data = dict()\n",
    "\n",
    "    for key in tqdm(data_dict.keys()):\n",
    "        #print(key)\n",
    "        window_data = []\n",
    "        sample = data_dict[key]\n",
    "        sample = sample[sample.isnull() == False]\n",
    "\n",
    "        High = sample.resample('1H').max()\n",
    "        Low = sample.resample('1H').min()\n",
    "        data_x = pd.concat([High,Low],axis=1)\n",
    "        count = len(High)\n",
    "        spilt_value = 8\n",
    "        spilt_count = int(count/spilt_value)\n",
    "        #print(spilt_count)\n",
    "\n",
    "        for i in range(spilt_count):\n",
    "            window_data.append(data_x.iloc[i*spilt_value:(i+1)*spilt_value])\n",
    "\n",
    "        # for i in range(count-spilt_value):\n",
    "        #     window_data.append(data_x.iloc[i:i+spilt_value])\n",
    "\n",
    "        label_data = []\n",
    "        for window_idx, window in enumerate(window_data):\n",
    "            label_window = []\n",
    "            for idx, element in enumerate(window.iloc):\n",
    "                label_value = []\n",
    "                for ele_idx, i in enumerate(element):\n",
    "                    value = 0\n",
    "                    if ((i >= value_3) & (i < value_4)) | ((i > value_1) & (i <= value_2)):\n",
    "                        value = 1\n",
    "                    # alarm\n",
    "                    if (i <= value_1) | (i >= value_4):\n",
    "                        value = 2\n",
    "                    if np.isnan(i) == True:\n",
    "                        break\n",
    "                    label_value.append(value)\n",
    "                    \n",
    "                if label_value :\n",
    "                    high = label_value[0]\n",
    "                    low = label_value[1]\n",
    "                    if (high == 2) | (low == 2):\n",
    "                        high = 2\n",
    "                    if ((high != 2) | (low != 2)) & ((high == 1) | (low == 1)):\n",
    "                        high = 1\n",
    "                    label_window.append(high)\n",
    "            if len(label_window) == 8:\n",
    "                label_data.append(label_window)\n",
    "\n",
    "        data[key] = label_data\n",
    "    return data\n",
    "\n",
    "def data_set_sea(data_dict, value_1,value_2):\n",
    "    data = dict()\n",
    "    for key in tqdm(data_dict.keys()):\n",
    "        #print(key)\n",
    "        window_data = []\n",
    "        sample = data_dict[key]\n",
    "        sample = sample[sample.isnull() == False]\n",
    "        label_data = []\n",
    "\n",
    "        High = sample.resample('1H').max()\n",
    "        count = len(High)\n",
    "        spilt_value = 8\n",
    "        for i in range(count-spilt_value):\n",
    "            window_data.append(High.iloc[i:i+spilt_value])\n",
    "\n",
    "        for window_idx, window in enumerate(window_data):\n",
    "            label_window = []\n",
    "            for idx, element in enumerate(window):\n",
    "                value = 0\n",
    "                # warning\n",
    "                if ((element >= value_1) & (element < value_2)):\n",
    "                    value = 1\n",
    "                # alarm\n",
    "                if (element >= value_2) :\n",
    "                    value = 2\n",
    "                if np.isnan(element) == True:\n",
    "                    break\n",
    "                label_window.append(value)\n",
    "            if len(label_window) == 8:\n",
    "                label_data.append(label_window)\n",
    "        data[key] = label_data\n",
    "\n",
    "    return data\n",
    "\n",
    "def data_label(data_dict):\n",
    "\n",
    "    label_dict = dict()\n",
    "    for key in tqdm(data_dict.keys()):\n",
    "        #print(key)\n",
    "        sample = data_dict[key]\n",
    "        label = np.zeros(len(sample), dtype=int)\n",
    "        for idx, window in enumerate(sample):\n",
    "            sum = np.mean(window)\n",
    "            if sum < 0.7:\n",
    "                label[idx] = 0\n",
    "            elif (sum < 1.6):\n",
    "                label[idx] = 1\n",
    "            else :\n",
    "                label[idx] = 2\n",
    "\n",
    "        label_dict[key] = [sample, label]\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def generate_data_set(conf):\n",
    "\twith open(conf.pre_pickle, 'rb') as f_1:\n",
    "\t\tdata_pre = pickle.load(f_1)\n",
    "\n",
    "\twith open(conf.pua_pickle, 'rb') as f_1:\n",
    "\t\tdata_pua = pickle.load(f_1)\n",
    "\n",
    "\twith open(conf.sea_pickle, 'rb') as f_1:\n",
    "\t\tdata_sea = pickle.load(f_1)\n",
    "\n",
    "\tdata_window_pre = data_set_pump(data_pre,1335,1467,1550,1625)\n",
    "\tdata_window_pua = data_set_pump(data_pua,450,566,1872,2232)\n",
    "\tdata_window_sea = data_set_sea(data_sea,5845,11036)\n",
    "\n",
    "\tdata_label_pre = data_label(data_window_pre)\n",
    "\tdata_label_pua = data_label(data_window_pua)\n",
    "\tdata_label_sea = data_label(data_window_sea)   \n",
    "\n",
    "\twith open(conf.pre_label_pickle, 'wb') as f_1:\n",
    "\t\tpickle.dump(data_label_pre,f_1)\n",
    "\n",
    "\twith open(conf.pua_label_pickle, 'wb') as f_1:\n",
    "\t\tpickle.dump(data_label_pua,f_1)\n",
    "\n",
    "\twith open(conf.sea_label_pickle, 'wb') as f_1:\n",
    "\t\tpickle.dump(data_label_sea,f_1)\n",
    "\n",
    "\treturn data_label_pre, data_label_pua, data_label_sea\n",
    "    \n",
    "#CheckP-Signals----------------------------------------------------------------------------------------------------------------------------\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False\n",
    "    \n",
    "def check_P_signals(p_df, col, conf, date_from=None, date_to=None, drawOp=False):\n",
    "    \n",
    "    if date_from == None:\n",
    "        date_from = p_df.iloc[0].name \n",
    "        date_to = p_df.iloc[-1].name\n",
    "        \n",
    "    p_value = conf.pvalue\n",
    "\n",
    "    for key in p_df.keys():\n",
    "\n",
    "        p_count = 0\n",
    "        warning_start_date = []\n",
    "        warning_end_date = []\n",
    "        alarm_start_date = []\n",
    "        alarm_end_date = []\n",
    "        temp_date = []\n",
    "        p_warn = pd.DataFrame()\n",
    "        p_alarm = pd.DataFrame()\n",
    "\n",
    "        result_path = './dtonic_result/P_signal/'+key\n",
    "        sample = p_df[key].loc[str(date_from):str(date_to)]\n",
    "        sample = sample.resample('1H').sum().apply(lambda x: x * -1 / 100 if x < 0 else x)\n",
    "           \n",
    "        for idx, value in zip(sample.index, sample):\n",
    "\n",
    "            if value == p_value[0]:\n",
    "                temp_date.append(idx)\n",
    "                p_count += 1\n",
    "\n",
    "            if value != p_value[0]:\n",
    "                if p_count >= p_value[0] and p_count < p_value[1]:\n",
    "                    warning_start_date.append(str(temp_date[-p_count]))\n",
    "                    warning_end_date.append(str(temp_date[-1]))\n",
    "\n",
    "                elif p_count >= p_value[1]:\n",
    "                    alarm_start_date.append(str(temp_date[-p_count]))\n",
    "                    alarm_end_date.append(str(temp_date[-1]))\n",
    "\n",
    "                p_count = 0\n",
    "                temp_date = []\n",
    "\n",
    "        for start,end in zip(warning_start_date,warning_end_date):\n",
    "            temp = sample[(sample.index >= start) & (sample.index <= end)]\n",
    "            p_warn = pd.concat([p_warn, temp], axis=0)\n",
    "\n",
    "        for start,end in zip(alarm_start_date,alarm_end_date):\n",
    "            temp = sample[(sample.index >= start) & (sample.index <= end)]\n",
    "            p_alarm = pd.concat([p_alarm, temp], axis=0)\n",
    "\n",
    "        month_start_date = []\n",
    "        month_end_date = []\n",
    "        \n",
    "        for idx, month_start in zip(sample.index, sample.index.is_month_start):\n",
    "            if month_start == True:\n",
    "                month_start_date.append((idx.date()))\n",
    "\n",
    "        for idx, month_end in zip(sample.index, sample.index.is_month_end):\n",
    "            if month_end == True:\n",
    "                month_end_date.append((idx.date()))\n",
    "\n",
    "\n",
    "        if (len(p_warn) > 0) | (len(p_alarm)) > 0:\n",
    "\n",
    "            result_path = Path(conf.result_data_path/'P_signal/')\n",
    "            result_path = os.path.join(result_path, key)\n",
    "            print(result_path)\n",
    "            createFolder(result_path)\n",
    "\n",
    "            if drawOp:\n",
    "                plt.ioff()\n",
    "                for start_day, end_day in zip(sorted(set(month_start_date)), sorted(set(month_end_date))):\n",
    "                    year, month, _ = str(start_day).split('-')\n",
    "                    fig, ax = plt.subplots(figsize= (20,10))\n",
    "                    ax.plot(sample.index, sample, alpha=0.2, color='blue')\n",
    "                    ax.scatter(p_alarm.index, p_alarm, color='red')\n",
    "                    ax.scatter(p_warn.index, p_warn, color='blue')\n",
    "                    ax.set_xlim([start_day, end_day])\n",
    "                    fig.autofmt_xdate()\n",
    "                    plt.title(\"P_signal : \" + month + \"월\")\n",
    "                    plt.savefig(result_path + '/' + key + '_' + year +'년'+month+'월'+'.png')\n",
    "                plt.close()\n",
    "\n",
    "            file = open(result_path+'/'+key + \"_P_signal.txt\", \"w\")\n",
    "            file.write(\" 점검지역 : \" + key + '\\n')\n",
    "            file.write(\" 점검기간 : {} ~ {}\\n\".format(str(date_from.date()), str(date_to.date())))\n",
    "            file.write(\" 점검사항 : 미수신 이벤트 [주의] {} 번, [경고] {} 번\\n\".format(len(warning_start_date), len(alarm_start_date)))\n",
    "            file.write('\\n 미수신 발생 주의 기간 : \\n')\n",
    "            for start,end in zip(warning_start_date,warning_end_date):\n",
    "                file.write(\" 시작 : \" + str(start)+' , '+\" 끝 :\"+str(end)+'\\n')\n",
    "            file.write('\\n 미수신 발생 경고 기간 : \\n')\n",
    "            for start,end in zip(alarm_start_date,alarm_end_date):\n",
    "                file.write(\" 시작 : \" + str(start)+' , '+\" 끝 :\"+str(end)+'\\n')\n",
    "            file.close()\n",
    "\n",
    "#HumidityAnalysis----------------------------------------------------------------------------------------------------------------------------\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "def humidity_analysis(hum_set, conf, date_from=None, date_to=None, drawOp=False):\n",
    "\n",
    "    hum_df, hum_med = hum_set\n",
    "    if date_from == None:\n",
    "        date_from = hum_df.iloc[0].name \n",
    "        date_to = hum_df.iloc[-1].name\n",
    "        \n",
    "    hum_value = conf.hum_value\n",
    "\n",
    "    for key in hum_df.columns:\n",
    "\n",
    "        warn_date = []\n",
    "        alarm_date = []\n",
    "        sample = hum_df[key].resample('1D').mean()\n",
    "        sample = sample.loc[str(date_from):str(date_to)]\n",
    "\n",
    "        x = sample - hum_med\n",
    "\n",
    "        check = x[(x >= hum_value[0]) | (x <= hum_value[2])]\n",
    "\n",
    "        if (len(check) > 0) :\n",
    "            result_path = Path(conf.result_data_path/'HUMIDITY/')\n",
    "            result_path = os.path.join(result_path, key)\n",
    "            print(result_path)\n",
    "            createFolder(result_path)            \n",
    "\n",
    "            for idx, value in zip(x.index,x):\n",
    "                value = round(value,2)\n",
    "                if (value >= hum_value[0] and value <= hum_value[1]) | (value > hum_value[3] and value <= hum_value[2]):\n",
    "                    warn_date.append(idx.date())\n",
    "                if (value > hum_value[1]) | (value <= hum_value[3]):\n",
    "                    alarm_date.append(idx.date())\n",
    "            hum_warn = sample[warn_date]\n",
    "            hum_alarm = sample[alarm_date]\n",
    "            \n",
    "            if drawOp:\n",
    "                fig, ax = plt.subplots(figsize= (20,10))\n",
    "                ax.plot(sample, alpha=0.2,color='black', label ='data')\n",
    "                ax.plot(hum_med, alpha=0.8, color='black',label='median')\n",
    "                ax.scatter(hum_warn.index,hum_warn,color='blue', label ='warn')\n",
    "                ax.scatter(hum_alarm.index,hum_alarm,color='red', label ='alarm')\n",
    "                ax.set_xlim([date_from.date(), date_to.date()])\n",
    "                fig.autofmt_xdate()\n",
    "\n",
    "                plt.title(\"Humidity\")\n",
    "                plt.legend()\n",
    "                plt.savefig(result_path + '/' + key + '_hum.png')\n",
    "                plt.close()\n",
    "\n",
    "            with open(result_path + '/' + key + \"_hum.txt\", \"w\") as file:\n",
    "                file.write(\" 점검지역 : \" + key + '\\n')\n",
    "                file.write(\" 점검기간 : {} ~ {}\\n\".format(str(date_from.date()), str(date_to.date())))\n",
    "                file.write(\" 점검사항 : 습도센서 이상 이벤트 [주의] {} 번, [경고] {} 번\\n\".format(len(hum_warn), len(hum_alarm)))\n",
    "                file.write('\\n 습도센서 이상징후 주의 기간 :')\n",
    "                for date in warn_date:\n",
    "                    file.write('\\n' + str(date))\n",
    "                file.write('\\n\\n 습도센서 이상징후 경고 기간 : ')\n",
    "                for date in alarm_date:\n",
    "                    file.write('\\n' + str(date))\n",
    "                    \n",
    "#TempAnalysis----------------------------------------------------------------------------------------------------------------------------\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "def temperature_analysis(temp_set, conf, date_from=None, date_to=None, drawOp=False):\n",
    "\n",
    "    temp_df, temp_med = temp_set\n",
    "    if date_from == None:\n",
    "        date_from = temp_df.iloc[0].name \n",
    "        date_to = temp_df.iloc[-1].name\n",
    "        \n",
    "    temp_value = conf.temp_value\n",
    "\n",
    "    for key in temp_df.columns:\n",
    "       \n",
    "        warn_date = []\n",
    "        alarm_date = []\n",
    "        sample = temp_df[key].resample('1D').mean()\n",
    "        sample = sample.loc[str(date_from):str(date_to)]\n",
    "\n",
    "        x = sample - temp_med\n",
    "\n",
    "        check = x[(x >= temp_value[0]) | (x <= temp_value[2])]\n",
    "\n",
    "        if len(check) > 0 :\n",
    "            result_path = Path(conf.result_data_path/'TEMP/')\n",
    "            result_path = os.path.join(result_path, key)\n",
    "            print(result_path)\n",
    "            createFolder(result_path)\n",
    "\n",
    "            for idx, value in zip(x.index,x):\n",
    "                value = round(value,2)\n",
    "                if (value >= temp_value[0] and value <= temp_value[1]) | (value > temp_value[3] and value <= temp_value[2]):\n",
    "                    warn_date.append(idx.date())\n",
    "                if (value > temp_value[1]) | (value <= temp_value[3]):\n",
    "                    alarm_date.append(idx.date())\n",
    "            temp_warn = sample[warn_date]\n",
    "            temp_alarm = sample[alarm_date]\n",
    "\n",
    "            if drawOp:\n",
    "                fig, ax = plt.subplots(figsize= (20,10))\n",
    "                plt.ioff()\n",
    "                ax.plot(sample, alpha=0.2,color='black', label ='data')\n",
    "                ax.plot(temp_med, alpha=0.8, color='black',label='median')\n",
    "                ax.scatter(temp_warn.index,temp_warn,color='blue', label ='warn')\n",
    "                ax.scatter(temp_alarm.index,temp_alarm,color='red', label ='alarm')\n",
    "                ax.set_xlim([date_from.date(), date_to.date()])\n",
    "                fig.autofmt_xdate()\n",
    "\n",
    "                plt.title('Temperature')\n",
    "                plt.legend()\n",
    "                plt.savefig(result_path + '/' + key + '_temp.png')\n",
    "                plt.close()\n",
    "\n",
    "            with open(result_path + '/' + key + \"_temp.txt\", \"w\") as file:\n",
    "                file.write(\" 점검지역 : \" + key + '\\n')\n",
    "                file.write(\" 점검기간 : {} ~ {}\\n\".format(str(date_from.date()), str(date_to.date())))\n",
    "                file.write(\" 점검사항 : 온도센서 이상 이벤트 [주의] {} 번, [경고] {} 번\\n\".format(len(temp_warn), len(temp_alarm)))\n",
    "                file.write('\\n 온도센서 이상징후 주의 기간 :')\n",
    "                for date in warn_date:\n",
    "                    file.write('\\n' +str(date))\n",
    "                file.write('\\n\\n 온도센서 이상징후 경고 기간 : ')\n",
    "                for date in alarm_date:\n",
    "                    file.write('\\n' +str(date))\n",
    "                    \n",
    "#pump_analysis----------------------------------------------------------------------------------------------------------------------------\n",
    "def set_label_timezone(timezone, conf):\n",
    "\n",
    "    Pre_alarm_LB = conf.pre_value[0]\n",
    "    Pre_warn_LB = conf.pre_value[1]\n",
    "    Pre_warn_UB = conf.pre_value[2]\n",
    "    Pre_alarm_UB = conf.pre_value[3]\n",
    "    \n",
    "    Pua_alarm_LB = conf.pua_value[0]\n",
    "    Pua_warn_LB = conf.pua_value[1]\n",
    "    Pua_warn_UB = conf.pua_value[2]\n",
    "    Pua_alarm_UB = conf.pua_value[3]\n",
    "\n",
    "    pre_label_dataset = [] # daily report only for timezone section\n",
    "    pua_label_dataset = [] # daily report only for timezone section\n",
    "    for idx in range(len(timezone)):\n",
    "        data_df = timezone[idx]\n",
    "        # Pump Presuure\n",
    "        pre_high = data_df['High(PRE)'].to_numpy()\n",
    "        pre_low = data_df['Low(PRE)'].to_numpy()\n",
    "        label_window = np.empty([8]) \n",
    "        for i, (v_high, v_low) in enumerate(zip(pre_high, pre_low)):\n",
    "            if np.isnan(v_high) or np.isnan(v_low):\n",
    "                label = 3 # P-signal\n",
    "            elif v_high > Pre_alarm_UB or v_low < Pre_alarm_LB:\n",
    "                label = 2\n",
    "            elif v_high > Pre_warn_UB or v_low < Pre_warn_LB:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            label_window[i] = int(label)             \n",
    "        pre_label_dataset.append(label_window.astype(int))\n",
    "\n",
    "        \n",
    "        #Pum Current\n",
    "        pua_high = data_df['High(PUA)'].to_numpy()\n",
    "        pua_low = data_df['Low(PUA)'].to_numpy()        \n",
    "        label_window = np.empty([8]) \n",
    "        for i, (v_high, v_low) in enumerate(zip(pua_high, pua_low)):\n",
    "            if np.isnan(v_high) or np.isnan(v_low):\n",
    "                label = 3 # P-signal\n",
    "            elif v_high > Pua_alarm_UB or v_low < Pua_alarm_LB:\n",
    "                label = 2\n",
    "            elif v_high > Pua_warn_UB or v_low < Pua_warn_LB:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            label_window[i] = int(label)           \n",
    "        pua_label_dataset.append(label_window.astype(int))    \n",
    "        \n",
    "    return pre_label_dataset, pua_label_dataset\n",
    "\n",
    "def ai_evaluation(model, data_array):\n",
    "    \n",
    "    psig_idx_list, eval_idx_list = [], []\n",
    "    for i, data in enumerate(data_array):\n",
    "        if max(data) == 3:\n",
    "            psig_idx_list.append(i)\n",
    "        else:\n",
    "            eval_idx_list.append(i)\n",
    "\n",
    "    eval_arr = np.empty((0,8), dtype=float)\n",
    "    for i in range(len(data_array)):\n",
    "        if i in eval_idx_list:\n",
    "            eval_arr = np.vstack((eval_arr, data_array[i]))\n",
    "    \n",
    "    label_data = np.reshape(eval_arr, (np.shape(eval_arr)[0], np.shape(eval_arr)[1], 1))\n",
    "    label_data = label_data/2\n",
    "    pred_Y = model.predict(label_data)\n",
    "    pred_y = []\n",
    "    for idx in range(len(pred_Y)):\n",
    "        y = pred_Y[idx]\n",
    "        y_index = np.argmax(y)\n",
    "        pred_y = np.append(pred_y, y_index)\n",
    "    predY = pred_y.astype(np.int32)\n",
    "\n",
    "    ai_report, cnt = [], 0\n",
    "    for idx in range(len(data_array)):\n",
    "        if idx in psig_idx_list:\n",
    "            result = 3 # p-signal\n",
    "        else:\n",
    "            result = predY[cnt]\n",
    "            cnt += 1\n",
    "        ai_report.append(result)\n",
    "    \n",
    "    return ai_report\n",
    "\n",
    "def pump_analysis(pre_df, pua_df, model, conf, date_from, date_to):\n",
    "    print(\"Starting Pump Analysis\")\n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    temp_list_1 = []\n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "\n",
    "    report_all = dict()\n",
    "    message = [\"None\", \"Warn\", \"Alarm\"]\n",
    "    Pre_alarm_LB = conf.pre_value[0]\n",
    "    Pre_warn_LB = conf.pre_value[1]\n",
    "    Pre_warn_UB = conf.pre_value[2]\n",
    "    Pre_alarm_UB = conf.pre_value[3]\n",
    "    \n",
    "    Pua_alarm_LB = conf.pua_value[0]\n",
    "    Pua_warn_LB = conf.pua_value[1]\n",
    "    Pua_warn_UB = conf.pua_value[2]\n",
    "    Pua_alarm_UB = conf.pua_value[3]\n",
    "\n",
    "    scount = 0\n",
    "    for key in pre_df.keys():\n",
    "        result_path = Path(conf.result_data_path/'PUMP/')\n",
    "        result_path = os.path.join(result_path, key)\n",
    "        \n",
    "        pre_sample = pre_df[key]\n",
    "        pre_sample = pre_sample.loc[str(date_from):str(date_to)]\n",
    "        \n",
    "        pua_sample = pua_df[key]\n",
    "        pua_sample = pua_sample.loc[str(date_from):str(date_to)]\n",
    "\n",
    "        pre_High = pre_sample.resample('1H').max()\n",
    "        pre_Low = pre_sample.resample('1H').min()\n",
    "        pua_High = pua_sample.resample('1H').max()\n",
    "        pua_Low = pua_sample.resample('1H').min()\n",
    "        \n",
    "        data_x = pd.concat([pre_High, pre_Low, pua_High, pua_Low], \n",
    "                           keys=['High(PRE)', 'Low(PRE)', 'High(PUA)', 'Low(PUA)'], axis=1)\n",
    "        data_x['datetime'] = pre_High.index\n",
    "        data_x['Date'] = [d.date() for d in data_x['datetime']]\n",
    "        data_x['Time'] = [d.time() for d in data_x['datetime']]\n",
    "        data_x = data_x.drop('datetime', axis=1)\n",
    "\n",
    "        count = len(pre_High)\n",
    "        timezone1 = [] # from 00H to 08H\n",
    "        timezone2 = [] # from 08H to 16H\n",
    "        timezone3 = [] # from 16H to 24H\n",
    "        date_string = []\n",
    "        for i in range(0, count, 24): # daily report\n",
    "            timezone1.append(data_x.iloc[i    : i+8])\n",
    "            timezone2.append(data_x.iloc[i+8  : i+16])\n",
    "            timezone3.append(data_x.iloc[i+16 : i+24])   \n",
    "            date_val = data_x['Date'].iloc[i]\n",
    "            date_string.append(date_val.strftime('%Y-%m-%d'))\n",
    "\n",
    "        pre_label_tzone1, pua_label_tzone1 = set_label_timezone(timezone1, conf)\n",
    "        pre_label_tzone2, pua_label_tzone2 = set_label_timezone(timezone2, conf)\n",
    "        pre_label_tzone3, pua_label_tzone3 = set_label_timezone(timezone3, conf)\n",
    "        \n",
    "        pre_tz1_report = ai_evaluation(model, pre_label_tzone1)\n",
    "        pre_tz2_report = ai_evaluation(model, pre_label_tzone2)\n",
    "        pre_tz3_report = ai_evaluation(model, pre_label_tzone3)\n",
    "        \n",
    "        pua_tz1_report = ai_evaluation(model, pua_label_tzone1)\n",
    "        pua_tz2_report = ai_evaluation(model, pua_label_tzone2)\n",
    "        pua_tz3_report = ai_evaluation(model, pua_label_tzone3)\n",
    "        \n",
    "        total_warns, total_alarms = 0, 0    \n",
    "        site_report = []\n",
    "        for idx, date_str in enumerate(date_string):\n",
    "            pre_tz1 = pre_tz1_report[idx]\n",
    "            pre_tz2 = pre_tz2_report[idx]\n",
    "            pre_tz3 = pre_tz3_report[idx]\n",
    "            pua_tz1 = pua_tz1_report[idx]\n",
    "            pua_tz2 = pua_tz2_report[idx]\n",
    "            pua_tz3 = pua_tz3_report[idx]\n",
    "            tz1_status = max(pre_tz1, pua_tz1)\n",
    "            tz2_status = max(pre_tz2, pua_tz2)\n",
    "            tz3_status = max(pre_tz3, pua_tz3)\n",
    "            found = False\n",
    "            daily_report = \"\"\n",
    "            if tz1_status > 0 and tz1_status < 3: \n",
    "                daily_report = \"[00H~08H] : \"\n",
    "                if pre_tz1 > 0:\n",
    "                    daily_report += \"{}(펌프압력) \".format(message[pre_tz1])\n",
    "                if pua_tz1 > 0:\n",
    "                    daily_report += \"{}(펌프전류) \".format(message[pua_tz1])\n",
    "                found = True\n",
    "            \n",
    "            if tz2_status > 0 and tz2_status < 3: \n",
    "                if found: daily_report += \", \"\n",
    "                daily_report += \"[08H~16H] : \"\n",
    "                if pre_tz2 > 0:\n",
    "                    daily_report += \"{}(펌프압력) \".format(message[pre_tz2])\n",
    "                if pua_tz2 > 0:\n",
    "                    daily_report += \"{}(펌프전류) \".format(message[pua_tz2])\n",
    "                found = True\n",
    "                                                       \n",
    "            if tz3_status > 0 and tz3_status < 3: \n",
    "                if found: daily_report += \", \"\n",
    "                daily_report += \"[16H~24H] : \"\n",
    "                if pre_tz3 > 0:\n",
    "                    daily_report += \"{}(펌프압력) \".format(message[pre_tz3])\n",
    "                if pua_tz3 > 0:\n",
    "                    daily_report += \"{}(펌프전류) \".format(message[pua_tz3])\n",
    "                                                       \n",
    "            daily_status = max(tz1_status, tz2_status, tz3_status)  \n",
    "            if daily_status == 0 or daily_status == 3: continue                                                \n",
    "            headline = \"{} : Daily Report = {}\\t{}\".format(date_str, message[daily_status], daily_report)\n",
    "            site_report.append(headline)                              \n",
    "            if daily_status == 1:\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                temp_bool = (key, date_str, message[daily_status], daily_report)\n",
    "                temp_list_1.append(temp_bool)\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                total_warns += 1\n",
    "            else:\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                temp_bool = (key, date_str, message[daily_status], daily_report)\n",
    "                temp_list_1.append(temp_bool)\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                total_alarms += 1\n",
    "            \n",
    "        # create a report file if this site has any issue(s)    \n",
    "        if 0.2*total_warns + total_alarms >= 1:\n",
    "            scount += 1\n",
    "            createFolder(result_path)\n",
    "            with open(result_path + '/' + key + \"_pump.txt\", \"w\") as file:\n",
    "                file.write(\" 점검지역 : \" + key + '\\n')\n",
    "                \n",
    "                print(\"* 점검지역 : {} >>> 점검사항 : 펌프 이상 이벤트 [주의] {} 번, [경고] {} 번\".format(\n",
    "                    key, total_warns, total_alarms) )\n",
    "                file.write(\"* 점검기간 : {} ~ {}\\n\".format(str(date_from.date()), str(date_to.date())))\n",
    "                file.write(\"* 점검사항 : 펌프 이상 이벤트 [주의] {} 번, [경고] {} 번\\n\".format(total_warns, total_alarms))\n",
    "                file.write(\"  Pump Press Mornitoring: LB(alarm)={:.1f}, LB(warn)={:.1f}, UB(warn)={:.1f}, UB(alarm)={:.1f}\\n\".format( \\\n",
    "                    Pre_alarm_LB, Pre_warn_LB, Pre_warn_UB, Pre_alarm_UB))\n",
    "                file.write(\"  Pump Current Mornitoring: LB(alarm)={:.1f}, LB(warn)={:.1f}, UB(warn)={:.1f}, UB(alarm)={:.1f}\\n\".format( \\\n",
    "                    Pua_alarm_LB, Pua_warn_LB, Pua_warn_UB, Pua_alarm_UB))\n",
    "                file.write('\\n* 펌프 이상징후 일간 보고 >>>\\n')\n",
    "                for headline in site_report:\n",
    "                    print(headline)\n",
    "                    file.write(headline+\"\\n\")    \n",
    "            \n",
    "            report_all[key] = [ (total_warns, total_alarms), headline ]\n",
    "        \n",
    "    print('total sites =', scount)\n",
    "    \n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    final_temp_list.append(temp_list_1)\n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "\n",
    "    return report_all\n",
    "\n",
    "#sensor_analysis----------------------------------------------------------------------------------------------------------------------------\n",
    "def set_sensor_label_timezone(timezone, conf):\n",
    "\n",
    "    warn_UB = conf.sea_value[0]\n",
    "    alarm_UB = conf.sea_value[1]\n",
    "    \n",
    "    label_dataset = [] # daily report only for timezone section\n",
    "    for idx in range(len(timezone)):\n",
    "        data_df = timezone[idx]\n",
    "        # Pump Presuure\n",
    "        high_amp = data_df['High'].to_numpy()\n",
    "        low_amp = data_df['Low'].to_numpy()\n",
    "        label_window = np.empty([8]) \n",
    "        for i, (v_high, v_low) in enumerate(zip(high_amp, low_amp)):\n",
    "            if np.isnan(v_high) or np.isnan(v_low):\n",
    "                label = 3 # P-signal\n",
    "            elif v_high > alarm_UB:\n",
    "                label = 2\n",
    "            elif v_high > warn_UB:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            label_window[i] = int(label)             \n",
    "        label_dataset.append(label_window.astype(int))\n",
    "                \n",
    "    return label_dataset\n",
    "\n",
    "def ai_evaluation(model, data_array):\n",
    "    \n",
    "    psig_idx_list, eval_idx_list = [], []\n",
    "    for i, data in enumerate(data_array):\n",
    "        if max(data) == 3:\n",
    "            psig_idx_list.append(i)\n",
    "        else:\n",
    "            eval_idx_list.append(i)\n",
    "\n",
    "    eval_arr = np.empty((0,8), dtype=float)\n",
    "    for i in range(len(data_array)):\n",
    "        if i in eval_idx_list:\n",
    "            eval_arr = np.vstack((eval_arr, data_array[i]))\n",
    "\n",
    "    label_data = np.reshape(eval_arr, (np.shape(eval_arr)[0], np.shape(eval_arr)[1], 1))\n",
    "    label_data = label_data/2\n",
    "    pred_Y = model.predict(label_data)\n",
    "    pred_y = []\n",
    "    for idx in range(len(pred_Y)):\n",
    "        y = pred_Y[idx]\n",
    "        y_index = np.argmax(y)\n",
    "        pred_y = np.append(pred_y, y_index)\n",
    "    predY = pred_y.astype(np.int32)\n",
    "\n",
    "    ai_report, cnt = [], 0\n",
    "    for idx in range(len(data_array)):\n",
    "        if idx in psig_idx_list:\n",
    "            result = 3 # p-signal\n",
    "        else:\n",
    "            result = predY[cnt]\n",
    "            cnt += 1\n",
    "        ai_report.append(result)\n",
    "    \n",
    "    return ai_report\n",
    "\n",
    "def sensor_analysis(sea_df, model, conf, date_from, date_to):\n",
    "    print(\"Starting Sensor Analysis\")\n",
    "    \n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    temp_list_1 = []\n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    report_all = dict()\n",
    "    message = [\"None\", \"Warn\", \"Alarm\"]\n",
    "    warn_UB = conf.sea_value[0]\n",
    "    alarm_UB = conf.sea_value[1]\n",
    "    scount = 0\n",
    "\n",
    "    for key in sea_df.keys():\n",
    "        result_path = Path(conf.result_data_path/'SENSOR/')\n",
    "        result_path = os.path.join(result_path, key)\n",
    "        \n",
    "        timezone1 = np.nan # from 00H to 08H\n",
    "        timezone2 = np.nan # from 08H to 16H\n",
    "        timezone3 = np.nan # from 16H to 24H\n",
    "        sample = sea_df[key]\n",
    "        sample = sample.loc[str(date_from):str(date_to)]\n",
    "        sample = sample[sample.isnull() == False]\n",
    "        High = sample.resample('1H').max()\n",
    "        Low = sample.resample('1H').min()\n",
    "        data_x = pd.concat([High, Low], keys=['High', 'Low'], axis=1)\n",
    "        data_x['datetime'] = High.index\n",
    "        data_x['Date'] = [d.date() for d in data_x['datetime']]\n",
    "        data_x['Time'] = [d.time() for d in data_x['datetime']]\n",
    "        data_x = data_x.drop('datetime', axis=1)\n",
    "            \n",
    "        count = len(High)\n",
    "        date_string = []\n",
    "        timezone1 = [] # from 00H to 08H\n",
    "        timezone2 = [] # from 08H to 16H\n",
    "        timezone3 = [] # from 16H to 24H\n",
    "        for i in range(0, count, 24): # daily report\n",
    "            timezone1.append(data_x.iloc[i    : i+8])\n",
    "            timezone2.append(data_x.iloc[i+8  : i+16])\n",
    "            timezone3.append(data_x.iloc[i+16 : i+24])   \n",
    "            date_val = data_x['Date'].iloc[i]\n",
    "            date_string.append(date_val.strftime('%Y-%m-%d'))\n",
    "\n",
    "        sea_label_tzone1 = set_sensor_label_timezone(timezone1, conf)\n",
    "        sea_label_tzone2 = set_sensor_label_timezone(timezone2, conf)\n",
    "        sea_label_tzone3 = set_sensor_label_timezone(timezone3, conf)\n",
    "\n",
    "        tz1_report = ai_evaluation(model, sea_label_tzone1)\n",
    "        tz2_report = ai_evaluation(model, sea_label_tzone2)\n",
    "        tz3_report = ai_evaluation(model, sea_label_tzone3)\n",
    "        \n",
    "        total_warns, total_alarms = 0, 0\n",
    "        alarm_list = []\n",
    "        message = [\"정상\", \"주의\", \"경고\"]\n",
    "        site_report = []\n",
    "        for idx, date_str in enumerate(date_string):\n",
    "            sea_tz1 = tz1_report[idx]\n",
    "            sea_tz2 = tz2_report[idx]\n",
    "            sea_tz3 = tz3_report[idx]\n",
    "            found = False\n",
    "            daily_report = \"\"\n",
    "            if sea_tz1 > 0 and sea_tz1 < 3: \n",
    "                daily_report += \"[00H~08H] : {}\".format(message[sea_tz1])\n",
    "                found = True\n",
    "            \n",
    "            if sea_tz2 > 0 and sea_tz2 < 3: \n",
    "                if found: daily_report += \", \"\n",
    "                daily_report += \"[08H~16H] : {}\".format(message[sea_tz2])\n",
    "                found = True\n",
    "                                                       \n",
    "            if sea_tz3 > 0 and sea_tz3 < 3: \n",
    "                if found: daily_report += \", \"\n",
    "                daily_report += \"[16H~24H] : {}\".format(message[sea_tz3])\n",
    "                                                       \n",
    "            daily_status = max(sea_tz1, sea_tz2, sea_tz3)  \n",
    "            if daily_status == 0 or daily_status == 3: continue                                                \n",
    "            headline = \"{} : Daily Report = (과전류){}\\t{}\".format(date_str, message[daily_status], daily_report)\n",
    "            site_report.append(headline)                              \n",
    "            if daily_status == 1:\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                temp_bool = (key, date_str, message[daily_status], daily_report)\n",
    "                temp_list_1.append(temp_bool)\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                total_warns += 1\n",
    "            else:\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                temp_bool = (key, date_str, message[daily_status], daily_report)\n",
    "                temp_list_1.append(temp_bool)\n",
    "                #beto did-------------------------------------------------------------------------------------------------\n",
    "                total_alarms += 1\n",
    "            \n",
    "        # create a report file if this site has any issue(s)    \n",
    "        if 0.2*total_warns + total_alarms >= 1:\n",
    "            scount += 1\n",
    "            createFolder(result_path)\n",
    "            with open(result_path + '/' + key + \"_sensor.txt\", \"w\") as file:\n",
    "                file.write(\" 점검지역 : \" + key + '\\n')\n",
    "                print(\" 점검지역 : {} >>> 점검사항 : 미세먼지 센서 이상 [주의] {} 번, [경고] {} 번\".format(\n",
    "                    key, total_warns, total_alarms) )\n",
    "                file.write(\" 점검기간 : {} ~ {}\\n\".format(str(date_from.date()), str(date_to.date())))\n",
    "                file.write(\" 점검사항 : 미세먼지 센서 과전류 [주의] {} 번, [경고] {} 번\\n\".format(total_warns, total_alarms))\n",
    "                file.write(\" 센서 과전류 감시 : UB(주의)={:.1f}, UB(경고)={:.1f}\\n\".format(warn_UB, alarm_UB))\n",
    "                file.write('\\n 미세먼지 센서 이상징후 일일 보고 >>>\\n')\n",
    "                for headline in site_report:\n",
    "                    print(headline)\n",
    "                    file.write(headline+\"\\n\")      \n",
    "                    \n",
    "            report_all[key] = [ (total_warns, total_alarms), site_report ]\n",
    "        \n",
    "    print('total sites =', scount)\n",
    "    \n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    final_temp_list.append(temp_list_1)\n",
    "    #beto did-------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return report_all\n",
    "    \n",
    "def RunCheckDevices(conf, date_from, date_to, output_filename):\n",
    "    #beto did---------------------------------\n",
    "    #final_temp_list = []\n",
    "    #beto did---------------------------------\n",
    "\n",
    "    events = get_event_list(conf)\n",
    "    event_list = convert_dataframe_to_list(events)\n",
    "    abnormal_list = [ event[0] for event in event_list ]\n",
    "\n",
    "    pre_df = load_data_pickle(conf, 'pump_pres', 'dtonic01')\n",
    "    pua_df = load_data_pickle(conf, 'pump_acur', 'dtonic01')\n",
    "    sea_df = load_data_pickle(conf, 'sensor_acur', 'dtonic01')\n",
    "\n",
    "    pre_df = set_time_period(pre_df, date_from, date_to)\n",
    "    pua_df = set_time_period(pua_df, date_from, date_to)\n",
    "    sea_df = set_time_period(sea_df, date_from, date_to)\n",
    "\n",
    "    model = load_model(conf.sensor_model_path)\n",
    "\n",
    "    os.system('cls')\n",
    "    print(\"\\n\\n미세먼지 센서 이상 분석\\n\")\n",
    "    sensor_report = sensor_analysis(sea_df, model, conf, date_from, date_to)\n",
    "    print(\"\\n펌프 장치 이상 분석\\n\")\n",
    "    pump_report = pump_analysis(pre_df, pua_df, model, conf, date_from, date_to)\n",
    "\n",
    "    true_negative_list = []\n",
    "    false_negative_list = deepcopy(abnormal_list)\n",
    "    false_positive_list = []\n",
    "    false_negative_list\n",
    "\n",
    "    for key, _ in sensor_report.items():\n",
    "        if key in abnormal_list:\n",
    "            true_negative_list.append(key)\n",
    "            for index, site in enumerate(false_negative_list):\n",
    "                if site == key:\n",
    "                    false_negative_list.pop(index)\n",
    "                    break\n",
    "        else:\n",
    "            false_positive_list.append(key)\n",
    "\n",
    "    for key, _ in pump_report.items():\n",
    "        if key in abnormal_list:\n",
    "            if key not in true_negative_list:\n",
    "                true_negative_list.append(key)\n",
    "            if key in false_negative_list:\n",
    "                for index, site in enumerate(false_negative_list):\n",
    "                    if site == key:\n",
    "                        false_negative_list.pop(index)\n",
    "                        break\n",
    "            if key in false_positive_list:\n",
    "                for index, site in enumerate(false_positive_list):\n",
    "                    if site == key:\n",
    "                        false_positive_list.pop(index)\n",
    "                        break            \n",
    "        else:\n",
    "            false_positive_list.append(key)\n",
    "\n",
    "    N = len(sea_df.keys())\n",
    "    TN = len(true_negative_list)\n",
    "    FP = len(false_positive_list)\n",
    "    FN = len(false_negative_list)\n",
    "    TP = N - TN - FP - FN\n",
    "\n",
    "    with open(\"true_negatives.txt\", \"w\") as f1:\n",
    "        str_report = '\\n'.join(map(str,true_negative_list))\n",
    "        f1.write(str_report)\n",
    "        f1.write(\"\\n총 {}개\".format(TN))\n",
    "\n",
    "    with open(\"false_negatives.txt\", \"w\") as f2:\n",
    "        str_report = '\\n'.join(map(str,false_negative_list))\n",
    "        f2.write(str_report)\n",
    "        f2.write(\"\\n총 {}개\".format(FN))\n",
    "\n",
    "    with open(\"false_positives.txt\", \"w\") as f3:\n",
    "        str_report = '\\n'.join(map(str,false_positive_list))\n",
    "        f3.write(str_report)\n",
    "        f3.write(\"\\n총 {}개\".format(FP))\n",
    "\n",
    "    Accuracy = (TP+TN)/N\n",
    "    print(TP)\n",
    "    print(TN)\n",
    "    \n",
    "    Precision_normal = TP / (TP + FN)\n",
    "    Recall_normal = TP / (TP + FP)\n",
    "    Precision_abnormal = TN / (FP + TN)\n",
    "    Recall_abnormal = TN / (FN + TN)    \n",
    "    precision = (Precision_normal+Precision_abnormal)/2\n",
    "    recall = (Recall_normal+Recall_abnormal)/2\n",
    "    f1_score = 2*precision*recall / (precision + recall)\n",
    "\n",
    "    print(\"\\noverall_report\\n\")\n",
    "    with open(\"overall_report.txt\", \"w\") as f4:\n",
    "        f4.write(\"TP={}, TN={}, FP={}, FN={}, All={}\\n\".format(TP,TN,FP,FN,N))\n",
    "        print(\"TP={}, TN={}, FP={}, FN={}, All={}\".format(TP,TN,FP,FN,N))\n",
    "        f4.write(\"Accuracy={:.4f}, Precisio={:.4f}, Recall={:.4f}\\n\".format(Accuracy, precision, recall))\n",
    "        print(\"Accuracy={:.4f}, Precisio={:.4f}, Recall={:.4f}\".format(Accuracy, precision, recall))\n",
    "        f4.write(\"F1-scorey={:.4f}\\n\".format(f1_score))\n",
    "        print(\"F1-scorey={:.4f}\".format(f1_score))\n",
    "\n",
    "    final_df = pd.DataFrame(columns=['Area','Date','Message','Time'])\n",
    "\n",
    "    for i in final_temp_list:\n",
    "        for y in tqdm(i):\n",
    "            final_df.loc[len(final_df)] = list(y)\n",
    "\n",
    "    for num in tqdm(range(len(final_df))):\n",
    "        a = final_df['Time'].iloc[num]\n",
    "        temp_dict = dict()\n",
    "        for i in a.split(','):\n",
    "            temp_dict[i.split(':')[0].strip()] = i.split(':')[1].strip()\n",
    "            for key in list(temp_dict.keys()):\n",
    "                final_df.at[num, key] = temp_dict[key]\n",
    "    final_df[['00H~08H', '08H~16H','16H~24H']] = final_df[['[00H~08H]','[08H~16H]','[16H~24H]']]\n",
    "    final_df = final_df[['Area', 'Date', 'Message','00H~08H', '08H~16H', '16H~24H']]\n",
    "    final_df.fillna(value='없음', inplace=True)\n",
    "    final_df.Message = final_df.Message.map({'Warn':'주의', 'Alarm':'경고', '경고':'경고', '주의':'주의'})      \n",
    "        \n",
    "    final_df.to_csv(output_filename)\n",
    "    print(\"report_file_loc: {}\".format(output_filename))\n",
    "\n",
    "####################################################################################\n",
    "############################Run All Modules for Analysis############################\n",
    "####################################################################################\n",
    "\n",
    "def RunAllModules(date_from, date_to, output_filename, config_filename):\n",
    "    yaml.dump(edict2dict(get_config(config_filename)), open('config.yaml', 'w'), default_flow_style=False)\n",
    "\n",
    "    # For Turning off the warning from pandas\n",
    "    pd.set_option('mode.chained_assignment',  None)\n",
    "\n",
    "    print('Dtonic Solution Service.\\n'+'Runs: Check_P_Signals\\n'+'#'*100)\n",
    "    #DataReader-------------------------------\n",
    "    date_from = datetime.strptime(date_from, '%Y-%m-%d')\n",
    "    date_to = datetime.strptime(date_to, '%Y-%m-%d')\n",
    "    conf = get_config(config_filename)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    #site_dict = load_site_data(conf)\n",
    "    #P_signal, temp, hum, pre, pua, sea, pm25, pm10 = get_all_data(site_dict, conf, date_from, date_to)\n",
    "    \n",
    "    # The anme of col is not important: anyone in ['PM2.5', 'PM10', 'TEMP', 'HUM', 'PRE', 'PUA', 'SAE'] \n",
    "    P_signal = load_data_pickle(conf, 'Temp', 'dtonic01')\n",
    "    P_signal = set_time_period(P_signal, date_from, date_to)\n",
    "    check_P_signals(P_signal, \"PM2.5\", conf, date_from, date_to, drawOp=False)\n",
    "\n",
    "    print('Dtonic Solution Service.\\n'+'Runs: Humidity_Analysis\\n'+'#'*100)\n",
    "    #-----------------------------------------\n",
    "    hum_df = load_data_pickle(conf, 'Humid', 'dtonic01')\n",
    "    hum_med = load_data_pickle(conf, 'Humid_median', 'dtonic01')\n",
    "    hum_df = set_time_period(hum_df, date_from, date_to)\n",
    "    hum_med = set_time_period(hum_med, date_from, date_to)\n",
    "    hum = (hum_df, hum_med)\n",
    "    humidity_analysis(hum, conf, date_from, date_to, drawOp=False)\n",
    "\n",
    "    print('Dtonic Solution Service.\\n'+'Runs: Temp_Analysis\\n'+'#'*100)\n",
    "    #-----------------------------------------\n",
    "    temp_df = load_data_pickle(conf, 'Temp', 'dtonic01')\n",
    "    temp_med = load_data_pickle(conf, 'Temp_median', 'dtonic01')\n",
    "    temp_df = set_time_period(temp_df, date_from, date_to)\n",
    "    temp_med = set_time_period(temp_med, date_from, date_to)\n",
    "    temp = (temp_df, temp_med)\n",
    "    temperature_analysis(temp, conf, date_from, date_to, drawOp=False)\n",
    "    \n",
    "    print('Dtonic Solution Service.\\n'+'Runs: Check Devices\\n'+'#'*100)\n",
    "    #-----------------------------------------\n",
    "    RunCheckDevices(conf, date_from, date_to, output_filename)\n",
    "\n",
    "##########################################################################        \n",
    "#################################AI Model#################################\n",
    "##########################################################################\n",
    "\n",
    "#-----------------------------------------------\n",
    "#\tLSTM Model Training/Test\n",
    "#-----------------------------------------------\n",
    "def make_dataset(data_dict):\n",
    "\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    flag =0\n",
    "    for key in data_dict.keys():\n",
    "        sample = data_dict[key]\n",
    "        if np.isnan(sample[0]).any(): continue\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            data_x = sample[0]\n",
    "            data_y = sample[1]\n",
    "        else :\n",
    "            data_x = np.concatenate([data_x,sample[0]],axis=0)\n",
    "            data_y = np.concatenate([data_y,sample[1]],axis=0)\n",
    "\n",
    "    print(np.shape(data_x))\n",
    "    print(np.shape(data_y))\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "def train_model(data_x, data_y, conf):\n",
    "\n",
    "    data_x = data_x/2.0\n",
    "\n",
    "    y_0 = np.where(data_y == 0)[0]\n",
    "    y_1 = np.where(data_y == 1)[0]\n",
    "    y_2 = np.where(data_y == 2)[0]\n",
    "    min_data_size = min(len(y_0), len(y_1), len(y_2))\n",
    "    \n",
    "    x_normal = data_x[y_0]\n",
    "    x_warn = data_x[y_1]\n",
    "    x_alarm = data_x[y_2]\n",
    "    shuffle(x_normal)\n",
    "    shuffle(x_warn)\n",
    "    shuffle(x_alarm)\n",
    "    x_normal = x_normal[:min_data_size]\n",
    "    x_warn = x_warn[:min_data_size]\n",
    "    x_alarm = x_alarm[:min_data_size]\n",
    "    \n",
    "    x_data = np.concatenate([x_normal,x_warn,x_alarm],axis=0)\n",
    "    y_data = [0]*len(x_normal)+[1]*len(x_warn)+[2]*len(x_alarm)\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = to_categorical(y_data,3)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "    \n",
    "    dev = '/cpu:0'\n",
    "    opt = Adam(learning_rate=0.01)\n",
    "\n",
    "    with tf.device(dev):  \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape =(8,1)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    with tf.device(dev):  \n",
    "        history = model.fit(x_train, y_train, conf.lstm_batch_size, conf.lstm_epochs, validation_data=(x_test,y_test), shuffle=True)\n",
    "        \n",
    "    plot_history(history.history)\n",
    "    plt.show()\n",
    "    \n",
    "    model.save(conf.sensor_model_path)\n",
    "    \n",
    "\n",
    "    return model, (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "\n",
    "    classY = []\n",
    "    for idx in range(len(y_test)):\n",
    "        y = y_test[idx]\n",
    "        y_index = np.argmax(y)\n",
    "        classY = np.append(classY, y_index)\n",
    "    realY = classY.astype(np.int32)\n",
    "    \n",
    "    pred_Y = model.predict(X_test, verbose=1)\n",
    "    \n",
    "    pred_y = []\n",
    "    for idx in range(len(pred_Y)):\n",
    "        y = pred_Y[idx]\n",
    "        y_index = np.argmax(y)\n",
    "        pred_y = np.append(pred_y, y_index)\n",
    "    predY = pred_y.astype(np.int32)\n",
    "    \n",
    "    cf = confusion_matrix(realY, predY)\n",
    "    print(cf)\n",
    "    \n",
    "    f1w = f1_score(realY, predY, average='weighted', zero_division=1)\n",
    "    print(\"f1-score is {}\".format(f1w))\n",
    "\n",
    " \n",
    "def load_all_labelled_data(conf):\n",
    "    with open(conf.pre_label_pickle, 'rb') as f_1:\n",
    "        data_label_pre = pickle.load(f_1)\n",
    "\n",
    "    with open(conf.pua_label_pickle, 'rb') as f_1:\n",
    "        data_label_pua = pickle.load(f_1)\n",
    "\n",
    "    with open(conf.sea_label_pickle, 'rb') as f_1:\n",
    "        data_label_sea = pickle.load(f_1)\n",
    "        \n",
    "    return data_label_pre, data_label_pua, data_label_sea\n",
    "\n",
    "\n",
    "#Function for running all codes above\n",
    "final_temp_list = []\n",
    "\n",
    "def RunEnkis(args):\n",
    "    pd.set_option('mode.chained_assignment',  None) # 경고 off\n",
    "    conf = get_config(args[\"config_filename\"])\n",
    "    if args[\"enable_preproc\"] == \"True\":\n",
    "        if args['start_again']=='True':\n",
    "            RunPreproc(conf)\n",
    "        if args[\"enable_preproc\"] == \"True\":\n",
    "            if 'location_dict.pickle' not in os.listdir('data/pickles/'): \n",
    "                print(\"Do the first phase of Preprocessing\")\n",
    "                RunPreproc(conf)\n",
    "            else: \n",
    "                print(\"Do the second phase of Preprocessing\")\n",
    "                RunPreprocWithPrevDict(conf)\n",
    "        \n",
    "        print(\"Splitting Data for Analysis\")\n",
    "        site_dict = load_site_data(conf)\n",
    "        P_signal, temp, hum, pre, pua, sea = get_all_data(site_dict, conf, args['date_from'],args['date_to'])\n",
    "        \n",
    "        print(\"Making Pickles for Training (SEA, PUA, PRE)\")\n",
    "        GenerateDataSet(args[\"config_filename\"])\n",
    "        \n",
    "    else:\n",
    "        print(\"Skip Preprocessing\")\n",
    "        #site_dict = load_site_data(conf)\n",
    "        #P_signal, temp, hum, pre, pua, sea = get_all_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        \n",
    "    if (args[\"mode\"] == \"evaluate\"):\n",
    "        print(\"Start Analysis\")\n",
    "        RunAllModules(date_from=args['date_from'], date_to=args[\"date_to\"], output_filename=args[\"output_filename\"], config_filename=args[\"config_filename\"])\n",
    "    else:\n",
    "        print(\"Unknown Mode: \" + args[\"mode\"])\n",
    "        \n",
    "def RunEnkisModelTraining(args):\n",
    "    pd.set_option('mode.chained_assignment',  None) # 경고 off\n",
    "    conf = get_config(args[\"config_filename\"])\n",
    "    conf.lstm_batch_size = 64\n",
    "    conf.lstm_epochs = 30\n",
    "    \n",
    "    if args[\"enable_preproc\"] == \"True\":\n",
    "        if args['start_again']=='True':\n",
    "            RunPreproc(conf)\n",
    "        if 'location_dict.pickle' not in os.listdir('data/pickles/'): \n",
    "            print(\"Do the first phase of Preprocessing\")\n",
    "            RunPreproc(conf)\n",
    "        else: \n",
    "            print(\"Do the second phase of Preprocessing\")\n",
    "            RunPreprocWithPrevDict(conf)\n",
    "            \n",
    "        print(\"Making Training Data\")\n",
    "        #-----------------------------------------\n",
    "        site_dict = load_site_data(conf)\n",
    "        pre = get_pump_pressure_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        pua = get_pump_current_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        sea = get_sensor_current_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        #-----------------------------------------\n",
    "    else:\n",
    "        print(\"Skip Preprocessing\")\n",
    "        site_dict = load_site_data(conf)\n",
    "        pre = get_pump_pressure_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        pua = get_pump_current_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "        sea = get_sensor_current_data(site_dict, conf, args[\"date_from\"], args[\"date_to\"])\n",
    "\n",
    "    if (args[\"mode\"] == \"train\"):\n",
    "        print(\"Start Training\")\n",
    "        #-----------------------------------------------\n",
    "        #\tGenerate Data Set for training/testing model\n",
    "        #-----------------------------------------------\n",
    "        data_label_pre, data_label_pua, data_label_sea = generate_data_set(conf)\n",
    "        \n",
    "        #-----------------------------------------------\n",
    "        #\tLSTM Model Training/Test||\n",
    "        #-----------------------------------------------\n",
    "        # data_label_pre, data_label_pua, data_label_sea = load_all_labelled_data(conf)\n",
    "        \n",
    "        # {{ ENKIS modified\n",
    "        data_pre_x, data_pre_y = make_dataset(data_label_pre)\n",
    "        data_pua_x, data_pua_y = make_dataset(data_label_pua)\n",
    "        data_sea_x, data_sea_y = make_dataset(data_label_sea)\n",
    "        \n",
    "        data_all_x = np.concatenate([data_pre_x, data_pua_x, data_sea_x], axis=0)\n",
    "        data_all_y = np.concatenate([data_pre_y, data_pua_y, data_sea_y], axis=0)\n",
    "\n",
    "        model, (X_Train, y_train), (X_test, y_test) = train_model(data_all_x, data_all_y, conf)\n",
    "        # }} end-of-ENKIS-modification\n",
    "\n",
    "        test_model(model, X_test, y_test)\n",
    "        \n",
    "        print(\"#\"*20)\n",
    "        print(\"#\"*20)\n",
    "        print(\"Model_Full_Path: {}\".format(conf.sensor_model_path))\n",
    "        print(\"#\"*20)\n",
    "        print(\"#\"*20)\n",
    "        \n",
    "    else:\n",
    "        print(\"Unknown Mode: \" + args[\"mode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd61a91-7edc-4912-b276-bbec77751d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/133 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:221: UserWarning: Workbook contains no stylesheet, using openpyxl's defaults\n",
      "  warn(\"Workbook contains no stylesheet, using openpyxl's defaults\")\n"
     ]
    }
   ],
   "source": [
    "RunEnkis({\"mode\": \"train\",\n",
    "          \"config_filename\":\"data_path.txt\",\n",
    "          \"enable_preproc\":\"True\",\n",
    "          \"output_filename\":\"sensor_pump_analysis.csv\",\n",
    "          \"date_from\": \"2022-02-01\",\n",
    "          \"date_to\": \"2022-07-30\",\n",
    "          \"start_again\":\"True\"\n",
    "          })\n",
    "\n",
    "# RunEnkisModelTraining({\"config_filename\":\"ai_voucher/enkis/data_path.txt\", \n",
    "#                       \"config_lstm_batch_size\":64,\n",
    "#                       \"config_lstm_epochs\":30,\n",
    "#                       \"enable_preproc\":\"False\",\n",
    "#                       \"mode\":\"train\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9840396-d40c-4373-9ef4-00bdb456c047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RunEnkisModelTraining({\"config_filename\":\"data_path.txt\", \n",
    "#                       \"config_lstm_batch_size\":64,\n",
    "#                       \"config_lstm_epochs\":30,\n",
    "#                       \"enable_preproc\":\"True\",\n",
    "#                       \"mode\":\"train\",\n",
    "#                       \"date_from\":\"none\",\n",
    "#                       \"date_to\":\"none\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb802d6-a2f1-440a-b0b4-8f62983542b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('data/original_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa72c9b-650c-4ca0-b50d-8e2cac9c00cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
